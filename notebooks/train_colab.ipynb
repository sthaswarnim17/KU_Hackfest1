{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e9882e",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision opencv-python-headless pillow pandas scikit-learn matplotlib seaborn tqdm -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4c7f7",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fba4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository or create structure\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('cache', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Project structure created\")\n",
    "print(\"\\nNext: Upload the source code files to Colab or mount Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1b2a3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Mount Google Drive (Optional)\n",
    "\n",
    "If your dataset and code are in Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to your project folder\n",
    "# %cd /content/drive/MyDrive/aero-gauge\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69460f43",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Upload or Download Dataset\n",
    "\n",
    "### Option A: Upload from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ecd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset zip file\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Uncomment to upload\n",
    "# uploaded = files.upload()\n",
    "# \n",
    "# # Unzip\n",
    "# for filename in uploaded.keys():\n",
    "#     if filename.endswith('.zip'):\n",
    "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#             zip_ref.extractall('.')\n",
    "#         print(f\"Extracted {filename}\")\n",
    "\n",
    "print(\"Upload completed (or skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6632bfc",
   "metadata": {},
   "source": [
    "### Option B: Download from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8154abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle\n",
    "!pip install kaggle -q\n",
    "\n",
    "# Upload your kaggle.json file first\n",
    "# Or manually create it:\n",
    "# from google.colab import files\n",
    "# files.upload()  # Upload kaggle.json\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download dataset\n",
    "!kaggle datasets download -d deadcardassian/pm25vision -p dataset/ --unzip\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded from Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0b06b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "!ls -lh dataset/\n",
    "\n",
    "# Check metadata\n",
    "if os.path.exists('dataset/train/metadata.csv'):\n",
    "    train_df = pd.read_csv('dataset/train/metadata.csv')\n",
    "    print(f\"\\nTrain samples: {len(train_df)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(train_df.head())\n",
    "    print(\"\\nColumns:\", train_df.columns.tolist())\n",
    "    \n",
    "    # Check PM2.5 distribution\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_df['pm2_5'].hist(bins=30)\n",
    "    plt.xlabel('PM2.5 (¬µg/m¬≥)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('PM2.5 Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    train_df['pm2_5'].plot(kind='box')\n",
    "    plt.ylabel('PM2.5 (¬µg/m¬≥)')\n",
    "    plt.title('PM2.5 Box Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è metadata.csv not found. Please check dataset path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6570f8c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Upload Source Code Files\n",
    "\n",
    "Upload the following files from your local `src/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0055f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload files manually\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload these files to 'src/' folder:\")\n",
    "print(\"- features.py\")\n",
    "print(\"- dataset.py\")\n",
    "print(\"- model.py\")\n",
    "print(\"- train.py\")\n",
    "print(\"- utils.py\")\n",
    "print(\"- inference.py\")\n",
    "\n",
    "# Uncomment to upload:\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     !mv {filename} src/\n",
    "\n",
    "# Option 2: If in Google Drive, copy from there\n",
    "# !cp /content/drive/MyDrive/aero-gauge/src/*.py src/\n",
    "\n",
    "print(\"\\n‚úÖ Source files ready\")\n",
    "!ls src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63053fe",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test Feature Extraction\n",
    "\n",
    "Quick test to ensure physics features work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11721ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from features import extract_all_features\n",
    "import numpy as np\n",
    "\n",
    "# Test with dummy image\n",
    "test_img = np.random.randint(100, 200, (224, 224, 3), dtype=np.uint8)\n",
    "features = extract_all_features(test_img)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(features)} physics features\")\n",
    "print(\"\\nSample features:\")\n",
    "for i, (key, value) in enumerate(list(features.items())[:5]):\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfae22",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Start Training\n",
    "\n",
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32  # Increase if GPU memory allows\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126329f",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "!python src/train.py \\\n",
    "    --data_dir ./dataset \\\n",
    "    --checkpoint_dir ./weights \\\n",
    "    --output_dir ./outputs \\\n",
    "    --cache_dir ./cache \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --num_workers {NUM_WORKERS} \\\n",
    "    --patience 5 \\\n",
    "    --freeze_backbone\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d140b",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training summary\n",
    "with open('outputs/training_summary.json', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Validation MAE: {summary['best_val_mae']:.2f} ¬µg/m¬≥\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "for key, value in summary['test_metrics'].items():\n",
    "    print(f\"  {key.upper()}: {value:.4f}\")\n",
    "\n",
    "# Display training curves\n",
    "if os.path.exists('outputs/training_results.png'):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    img = Image.open('outputs/training_results.png')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Results plot not found\")\n",
    "\n",
    "print(\"\\n‚úÖ Model saved to: weights/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c336c1",
   "metadata": {},
   "source": [
    "## üîü Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb44c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import create_predictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load predictor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "predictor = create_predictor(\n",
    "    checkpoint_dir='./weights',\n",
    "    output_dir='./outputs',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Predictor loaded\")\n",
    "\n",
    "# Test on a sample image from test set\n",
    "test_images = [f for f in os.listdir('dataset/test/images') if f.endswith(('.jpg', '.png'))][:3]\n",
    "\n",
    "print(f\"\\nTesting on {len(test_images)} sample images...\\n\")\n",
    "\n",
    "for img_file in test_images:\n",
    "    img_path = os.path.join('dataset/test/images', img_file)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Predict\n",
    "    result = predictor.predict(image)\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(f\"{img_file}\", fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(result['dark_channel_heatmap'])\n",
    "    axes[1].set_title('Dark Channel Heatmap', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f\"PM2.5: {result['pm25']:.1f} ¬µg/m¬≥ | AQI: {result['aqi_index']} ({result['aqi_category']})\",\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop features for {img_file}:\")\n",
    "    for feat in result['top_features'][:3]:\n",
    "        print(f\"  - {feat['name']}: {feat['raw_value']:.3f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Inference test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74b7bb",
   "metadata": {},
   "source": [
    "## üì• Download Trained Model\n",
    "\n",
    "Download the model and normalization stats to use locally or in Streamlit app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip with model and stats\n",
    "shutil.make_archive('aerogauge_model', 'zip', '.', 'weights')\n",
    "shutil.make_archive('aerogauge_outputs', 'zip', '.', 'outputs')\n",
    "\n",
    "# Download\n",
    "files.download('aerogauge_model.zip')\n",
    "files.download('aerogauge_outputs.zip')\n",
    "\n",
    "print(\"‚úÖ Model files downloaded\")\n",
    "print(\"\\nExtract these files locally:\")\n",
    "print(\"  - aerogauge_model.zip ‚Üí extract to your local 'weights/' folder\")\n",
    "print(\"  - aerogauge_outputs.zip ‚Üí extract to your local 'outputs/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea12a98",
   "metadata": {},
   "source": [
    "## üéâ Next Steps\n",
    "\n",
    "1. ‚úÖ Training completed on Colab GPU\n",
    "2. ‚úÖ Model and stats downloaded\n",
    "3. ‚úÖ Ready to deploy Streamlit app\n",
    "\n",
    "### To run the Streamlit app locally:\n",
    "\n",
    "```bash\n",
    "# Place downloaded files:\n",
    "# - weights/best_model.pt\n",
    "# - outputs/feature_normalization.json\n",
    "\n",
    "# Run app\n",
    "streamlit run src/app.py\n",
    "```\n",
    "\n",
    "### Performance Tips:\n",
    "\n",
    "- **Improve accuracy**: Increase epochs, try different learning rates\n",
    "- **Reduce overfitting**: Add more augmentation, increase dropout\n",
    "- **Faster training**: Increase batch size (if GPU memory allows)\n",
    "- **Better generalization**: Collect more diverse training data\n",
    "\n",
    "---\n",
    "\n",
    "**Happy training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
